{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "\n",
    "w_dir = %pwd\n",
    "work_dir = os.path.dirname(w_dir)\n",
    "work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(w_dir+'/fgc_support_retri')\n",
    "from fgc_support_retri.utils import *\n",
    "from fgc_support_retri import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_load(config.FGC_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_datas(data, fold):\n",
    "    fold_len = int(len(data)/fold)\n",
    "    fold_datas = []\n",
    "    for i in range(fold):\n",
    "        if i == fold-1:\n",
    "            fold_data = data[fold_len*i:]\n",
    "        else:\n",
    "            fold_data = data[fold_len*i:fold_len * (i+1)]\n",
    "        fold_datas.append(fold_data)\n",
    "    return fold_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_datas = get_fold_datas(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fold_datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 07:31:16.839226 140363791722304 file_utils.py:39] PyTorch version 1.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from fgc_support_retri.train import *\n",
    "\n",
    "def train_entity_model(num_epochs, batch_size, model_file_name, train_documents, lr):\n",
    "    dataset_reader = SerSentenceDataset\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    pretrained_bert = BertModel.from_pretrained(bert_model_name)\n",
    "    pretrained_bert.eval()\n",
    "    \n",
    "    model = EntitySERModel.from_pretrained(bert_model_name)\n",
    "    model.to_mode('etype+idf')\n",
    "    \n",
    "    collate_fn = Sent_collate\n",
    "    indexer = SentIdx(tokenizer, pretrained_bert)\n",
    "    input_names = ['input_ids', 'question_ids', 'token_type_ids', 'attention_mask', \n",
    "                   'tf_type', 'idf_type', 'sf_type', 'sf_score', 'qsim_type', 'atype_label', 'etype_ids', 'label']\n",
    "    trainer = SER_Trainer(model, collate_fn, indexer, dataset_reader, input_names, lr)\n",
    "    trainer.train(num_epochs, batch_size, model_file_name, train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0325 07:31:18.984321 140363791722304 tokenization_utils.py:375] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "I0325 07:31:19.929808 140363791722304 configuration_utils.py:152] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.3767c74c8ed285531d04153fe84a0791672aff52f7249b27df341dbce09b8305\n",
      "I0325 07:31:19.932777 140363791722304 configuration_utils.py:169] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0325 07:31:20.877711 140363791722304 modeling_utils.py:387] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "I0325 07:31:24.055299 140363791722304 configuration_utils.py:152] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.3767c74c8ed285531d04153fe84a0791672aff52f7249b27df341dbce09b8305\n",
      "I0325 07:31:24.060034 140363791722304 configuration_utils.py:169] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0325 07:31:24.985461 140363791722304 modeling_utils.py:387] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "I0325 07:31:27.170180 140363791722304 modeling_utils.py:457] Weights of EntitySERModel not initialized from pretrained model: ['bert.embeddings.tf_embeddings.weight', 'bert.embeddings.idf_embeddings.weight', 'bert.embeddings.etype_embeddings.weight', 'classifier.weight', 'classifier.bias']\n",
      "I0325 07:31:27.171780 140363791722304 modeling_utils.py:460] Weights from pretrained model not used in EntitySERModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "  0%|          | 0/247 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_set indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:17<00:00,  1.17it/s]\n",
      "100%|██████████| 794/794 [00:00<00:00, 13044.36it/s]\n",
      "  0%|          | 0/794 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set indexing...\n",
      "{'text': '電視名人。為川普集團前任董事長兼總裁', 'text_cn': '电视名人。为川普集团前任董事长兼总裁', 'start': 76, 'end': 94}\n",
      "{'text': '川普娛樂公司的創辦人，在全世界經營房地產', 'text_cn': '川普娱乐公司的创办人，在全世界经营房地产', 'start': 95, 'end': 115}\n",
      "{'text': '熱，電，輻射', 'text_cn': '热，电，辐射', 'start': 22, 'end': 28}\n",
      "{'text': '', 'text_cn': '', 'start': 0, 'end': 0}\n",
      "{'text': '校園注意教室之空氣流通，落實勤洗手，有呼吸道症狀時應配戴口罩；打噴嚏時應用面紙', 'text_cn': '校园注意教室之空气流通，落实勤洗手，有呼吸道症狀时应配戴口罩；打喷嚏时应用面纸', 'start': 88, 'end': 127}\n",
      "{'text': '他人交談時，儘可能保持1公尺以上之距離，避免病毒傳播。', 'text_cn': '他人交谈时，尽可能保持1公尺以上之距离，避免病毒传播。', 'start': 143, 'end': 170}\n",
      "{'text': '', 'text_cn': '', 'start': 0, 'end': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [11:24<00:00,  1.65it/s]\n",
      "  0%|          | 0/2423 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader...\n",
      "start training ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|██████████| 2423/2423 [07:57<00:00,  5.30it/s]\n",
      "  1%|          | 2/239 [00:00<00:12, 19.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train_loss: 0.173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_em': 0.146, 'sp_prec': 0.554, 'sp_recall': 0.658, 'sp_f1': 0.542}\n",
      "epoch 0 eval_recall: 0.658 eval_f1: 0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2423/2423 [07:54<00:00,  5.57it/s]\n",
      "  1%|          | 2/239 [00:00<00:12, 19.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train_loss: 0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:18<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_em': 0.172, 'sp_prec': 0.572, 'sp_recall': 0.628, 'sp_f1': 0.543}\n",
      "epoch 1 eval_recall: 0.628 eval_f1: 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2423/2423 [07:55<00:00,  5.27it/s]\n",
      "  1%|          | 2/239 [00:00<00:12, 18.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train_loss: 0.073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:18<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_em': 0.151, 'sp_prec': 0.585, 'sp_recall': 0.509, 'sp_f1': 0.504}\n",
      "epoch 2 eval_recall: 0.509 eval_f1: 0.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2423/2423 [07:55<00:00,  4.88it/s]\n",
      "  1%|          | 2/239 [00:00<00:13, 17.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train_loss: 0.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:19<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_em': 0.159, 'sp_prec': 0.562, 'sp_recall': 0.679, 'sp_f1': 0.558}\n",
      "epoch 3 eval_recall: 0.679 eval_f1: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2423/2423 [07:59<00:00,  5.44it/s]\n",
      "  1%|          | 2/239 [00:00<00:12, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train_loss: 0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:18<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_em': 0.163, 'sp_prec': 0.632, 'sp_recall': 0.587, 'sp_f1': 0.557}\n",
      "epoch 4 eval_recall: 0.587 eval_f1: 0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2423/2423 [07:53<00:00,  5.28it/s]\n",
      "  1%|          | 2/239 [00:00<00:12, 19.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train_loss: 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_em': 0.155, 'sp_prec': 0.605, 'sp_recall': 0.633, 'sp_f1': 0.561}\n",
      "epoch 5 eval_recall: 0.633 eval_f1: 0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2423/2423 [07:53<00:00,  5.46it/s]\n",
      "  1%|          | 2/239 [00:00<00:13, 18.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 train_loss: 0.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:19<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_em': 0.134, 'sp_prec': 0.565, 'sp_recall': 0.668, 'sp_f1': 0.557}\n",
      "epoch 6 eval_recall: 0.668 eval_f1: 0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2423/2423 [07:54<00:00,  5.46it/s]\n",
      "  1%|          | 2/239 [00:00<00:13, 18.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 train_loss: 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:19<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sp_em': 0.159, 'sp_prec': 0.593, 'sp_recall': 0.592, 'sp_f1': 0.54}\n",
      "epoch 7 eval_recall: 0.592 eval_f1: 0.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 953/2423 [03:06<04:43,  5.19it/s]"
     ]
    }
   ],
   "source": [
    "for i, predict_document in enumerate(fold_datas):\n",
    "    train_documents = []\n",
    "    for j, fold_data in enumerate(fold_datas):\n",
    "        if j != i:\n",
    "            train_documents += fold_data\n",
    "    train_entity_model(10, 12, \"fold_{}_entity\".format(i), train_documents, 2e-5)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
