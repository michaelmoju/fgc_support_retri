{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 10:13:32.902130 140581297116992 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1125 10:13:33.234193 140581297116992 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/work'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torchvision\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "\n",
    "w_dir = %pwd\n",
    "work_dir = os.path.dirname(w_dir)\n",
    "work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(w_dir+'/fgc_support_retri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "import config\n",
    "from fgc_preprocess import FgcSerDataset, BertIdx\n",
    "from torch.utils.data import DataLoader\n",
    "from sup_model import BertSupSentClassification\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "import config\n",
    "from fgc_preprocess import FgcSerDataset, BertIdx\n",
    "from torch.utils.data import DataLoader\n",
    "from sup_model import BertSupSentClassification\n",
    "from transformers import BertModel\n",
    "from transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "from tqdm import tqdm\n",
    "import ujson\n",
    "import json\n",
    "import config\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 10:25:55.222210 140581297116992 corenlp.py:42] Using an existing server http://140.109.19.191:9000\n",
      "I1125 10:25:56.226137 140581297116992 corenlp.py:118] The server is available.\n"
     ]
    }
   ],
   "source": [
    "nlp = StanfordCoreNLP('http://140.109.19.191', port=9000, lang='zh')\n",
    "props = {'annotators': 'ssplit', 'ssplit.boundaryTokenRegex': '[。]|[!?！？]+',\n",
    "         'outputFormat': 'json', 'pipelineLanguage': 'zh', 'timeout': '5000000'}\n",
    "\n",
    "def sentence_split(dtext):\n",
    "    anno = json.loads(nlp.annotate(dtext))\n",
    "\n",
    "    out_sents = []\n",
    "    for s in anno['sentences']:\n",
    "        s_start = s['tokens'][0]['characterOffsetBegin']\n",
    "        s_end = s['tokens'][-1]['characterOffsetEnd']\n",
    "        s_string = dtext[s_start: s_end]\n",
    "\n",
    "        out_sents.append(s_string)\n",
    "\n",
    "    return out_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2ids(tokenizer, sentences, question):\n",
    "    input_ids_all = torch.zeros((len(sentences), 512), dtype=torch.long)\n",
    "    token_type_ids_all = torch.zeros((len(sentences), 512), dtype=torch.long)\n",
    "    attention_mask_all = torch.zeros((len(sentences), 512), dtype=torch.long)\n",
    "    \n",
    "    for s_i, sentence in enumerate(sentences):\n",
    "        input_ids = torch.zeros(512, dtype=torch.long)\n",
    "        token_type_ids = torch.zeros(512, dtype=torch.long)\n",
    "        attention_mask = torch.zeros(512, dtype=torch.long)\n",
    "\n",
    "        tokenized_q = ['[CLS]'] + tokenizer.tokenize(question) + ['[SEP]']\n",
    "        tokenized_all = tokenized_q + tokenizer.tokenize(sentence)\n",
    "\n",
    "        if len(tokenized_all) > 511:\n",
    "            print(\"tokenized all > 511\")\n",
    "            tokenized_all = tokenized_all[:512]\n",
    "        tokenized_all += ['[SEP]']\n",
    "        ids_all = tokenizer.convert_tokens_to_ids(tokenized_all)\n",
    "\n",
    "        input_ids[:len(ids_all)] = torch.tensor(ids_all)\n",
    "        token_type_ids[len(tokenized_q):len(tokenized_all)] = 1\n",
    "        attention_mask[:len(tokenized_all)] = 1\n",
    "        \n",
    "        input_ids_all[s_i] = input_ids\n",
    "        token_type_ids_all[s_i] = token_type_ids\n",
    "        attention_mask_all[s_i] = attention_mask\n",
    "\n",
    "    return input_ids_all, token_type_ids_all, attention_mask_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SER_extract:\n",
    "    def __init__(self):\n",
    "        bert_model_name = 'bert-base-chinese'\n",
    "        bert_encoder = BertModel.from_pretrained(bert_model_name)\n",
    "        model = BertSupSentClassification(bert_encoder)\n",
    "        model.load_state_dict(torch.load('trainedmodel.m'))\n",
    "        model.eval()\n",
    "        \n",
    "        self.tokenizer = tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.model = model\n",
    "        \n",
    "    def predict(self, context_sents, question):\n",
    "        input_ids, token_type_ids, attention_mask = convert2ids(self.tokenizer, context_sents, question)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids, token_type_ids=token_type_ids,\n",
    "\t\t\t\t\t\t attention_mask=attention_mask, mode=BertSupSentClassification.ForwardMode.EVAL)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 10:30:00.518281 140581297116992 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.0c16faba8be66db3f02805c912e4cf94d3c9cffc1f12fa1a39906f9270f76d33\n",
      "I1125 10:30:00.520668 140581297116992 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I1125 10:30:01.454680 140581297116992 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "I1125 10:30:04.839547 140581297116992 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "ser_extracter = SER_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.FGC_TRAIN, 'r') as f:\n",
    "    items = ujson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fgc_preprocess import prepro_all\n",
    "def eval():\n",
    "    items = prepro_all(config.FGC_TRAIN)\n",
    "    \n",
    "    tp = 0\n",
    "    gol_t = 0\n",
    "    gol_t = 0\n",
    "    \n",
    "    \n",
    "    for item in items:\n",
    "        context_sents = sentence_split(item['DTEXT'])\n",
    "        for question in item['QUESTIONS']:\n",
    "            logits = ser_extracter.predict(context_sents, question)\n",
    "            prediction = torch.sigmoid(logits) > 0.5\n",
    "            prediction = prediction.squeeze(1).numpy()\n",
    "            gold = np.array(question['SUP_EVIDENCE'])\n",
    "            \n",
    "            gol_t = np.count_nonzero(gold == 1)\n",
    "            pre_t =  np.count_nonzero(prediction == 1)\n",
    "            \n",
    "            for i, gs in enumerate(g):\n",
    "                if gs == t[i] == 1:\n",
    "                    tp += 1\n",
    "            \n",
    "    print('prection = {}'.format(tp / pre_t))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = prediction.squeeze(1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "gol = 0\n",
    "pre = 0\n",
    "\n",
    "gol_t = np.count_nonzero(np.array(g) == 1)\n",
    "pre_t =  np.count_nonzero(t) == 1)\n",
    "\n",
    "for i, gs in enumerate(g):\n",
    "    if gs == t[i] == 1:\n",
    "        tp += 1\n",
    "recall = tp / gol_t\n",
    "precision = tp / pre_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
